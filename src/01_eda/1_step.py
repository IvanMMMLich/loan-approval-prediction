"""
===============================================================================
                    üí≥ LOAN APPROVAL PREDICTION
                    
                 –®–ê–ì 1: –ü–ï–†–í–û–ï –ó–ù–ê–ö–û–ú–°–¢–í–û –° –î–ê–ù–ù–´–ú–ò
                        "Understanding Credit Data"
===============================================================================

–ö–û–ù–¢–ï–ö–°–¢ –ó–ê–î–ê–ß–ò:
----------------
–ë–∞–Ω–∫ —Ö–æ—á–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–¥–æ–±—Ä–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–æ–≤. –ù—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å
–º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç, –±—É–¥–µ—Ç –ª–∏
–∫—Ä–µ–¥–∏—Ç –æ–¥–æ–±—Ä–µ–Ω –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω.

–ß–¢–û –ú–´ –£–ó–ù–ê–ï–ú –í –≠–¢–û–ú –§–ê–ô–õ–ï:
--------------------------
1. –†–∞–∑–º–µ—Ä –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
2. –¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—á–∏—Å–ª–æ–≤—ã–µ vs –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ)
3. –ü–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
4. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
"""

import pandas as pd
import numpy as np
from pathlib import Path
from colorama import init, Fore, Style
import warnings

warnings.filterwarnings('ignore')
init(autoreset=True)

# –ü—É—Ç–∏
ROOT_DIR = Path(__file__).parent.parent.parent
DATA_RAW = ROOT_DIR / 'data' / 'raw'
RESULTS = ROOT_DIR / 'results'

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –∫—Ä–µ–¥–∏—Ç–∞—Ö."""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üìÇ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –û –ö–†–ï–î–ò–¢–ê–•")
    print(f"{Fore.CYAN}{'='*80}")
    
    train_path = DATA_RAW / 'train.csv'
    test_path = DATA_RAW / 'test.csv'
    
    print(f"\n‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ...")
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    
    print(f"{Fore.GREEN}‚úÖ Train –∑–∞–≥—Ä—É–∂–µ–Ω: {train_df.shape}")
    print(f"{Fore.GREEN}‚úÖ Test –∑–∞–≥—Ä—É–∂–µ–Ω: {test_df.shape}")
    
    return train_df, test_df

def analyze_structure(train_df, test_df):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ –∫—Ä–µ–¥–∏—Ç–∞—Ö."""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üè¶ –°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–• –û –ö–†–ï–î–ò–¢–ê–•")
    print(f"{Fore.CYAN}{'='*80}")
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤
    personal_features = ['person_age', 'person_income', 'person_home_ownership', 'person_emp_length']
    loan_features = ['loan_intent', 'loan_grade', 'loan_amnt', 'loan_int_rate', 'loan_percent_income']
    credit_history = ['cb_person_default_on_file', 'cb_person_cred_hist_length']
    
    print(f"\nüë§ –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞:")
    for col in personal_features:
        if col in train_df.columns:
            dtype = train_df[col].dtype
            unique = train_df[col].nunique()
            print(f"   ‚Ä¢ {col:30} | –¢–∏–ø: {dtype} | –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {unique}")
    
    print(f"\nüí∞ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫—Ä–µ–¥–∏—Ç–∞:")
    for col in loan_features:
        if col in train_df.columns:
            dtype = train_df[col].dtype
            unique = train_df[col].nunique()
            print(f"   ‚Ä¢ {col:30} | –¢–∏–ø: {dtype} | –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {unique}")
    
    print(f"\nüìä –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è:")
    for col in credit_history:
        if col in train_df.columns:
            dtype = train_df[col].dtype
            unique = train_df[col].nunique()
            print(f"   ‚Ä¢ {col:30} | –¢–∏–ø: {dtype} | –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {unique}")
    
    print(f"\nüéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è:")
    print(f"   ‚Ä¢ loan_status (0=–æ—Ç–∫–∞–∑, 1=–æ–¥–æ–±—Ä–µ–Ω–∏–µ)")

def analyze_target(train_df):
    """–ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π loan_status."""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üéØ –ê–ù–ê–õ–ò–ó –û–î–û–ë–†–ï–ù–ò–Ø –ö–†–ï–î–ò–¢–û–í")
    print(f"{Fore.CYAN}{'='*80}")
    
    target_counts = train_df['loan_status'].value_counts()
    target_pct = train_df['loan_status'].value_counts(normalize=True) * 100
    
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ loan_status:")
    print(f"   –û—Ç–∫–∞–∑–∞–Ω–æ (0): {target_counts[0]:,} ({target_pct[0]:.1f}%)")
    print(f"   –û–¥–æ–±—Ä–µ–Ω–æ (1): {target_counts[1]:,} ({target_pct[1]:.1f}%)")
    
    approval_rate = target_pct[1]
    print(f"\nüí° –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–¥–æ–±—Ä–µ–Ω–∏—è: {approval_rate:.1f}%")
    
    if approval_rate < 30:
        print(f"   {Fore.RED}‚ö†Ô∏è –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–¥–æ–±—Ä–µ–Ω–∏—è - —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å!")
    elif approval_rate < 40:
        print(f"   {Fore.YELLOW}‚ö†Ô∏è –ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–¥–æ–±—Ä–µ–Ω–∏—è - –µ—Å—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    else:
        print(f"   {Fore.GREEN}‚úÖ –£–º–µ—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–¥–æ–±—Ä–µ–Ω–∏—è")

def analyze_features(train_df):
    """–ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üìä –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ò–ó–ù–ê–ö–û–í")
    print(f"{Fore.CYAN}{'='*80}")
    
    # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols.remove('id')
    if 'loan_status' in numeric_cols:
        numeric_cols.remove('loan_status')
    
    print(f"\nüìà –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    for col in numeric_cols:
        mean_val = train_df[col].mean()
        median_val = train_df[col].median()
        min_val = train_df[col].min()
        max_val = train_df[col].max()
        null_count = train_df[col].isnull().sum()
        
        print(f"\n   {col}:")
        print(f"      –î–∏–∞–ø–∞–∑–æ–Ω: [{min_val:.1f} - {max_val:.1f}]")
        print(f"      –°—Ä–µ–¥–Ω–µ–µ: {mean_val:.1f}, –ú–µ–¥–∏–∞–Ω–∞: {median_val:.1f}")
        if null_count > 0:
            print(f"      {Fore.YELLOW}–ü—Ä–æ–ø—É—Å–∫–∏: {null_count} ({null_count/len(train_df)*100:.1f}%)")
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
    
    if categorical_cols:
        print(f"\nüìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        for col in categorical_cols:
            unique_values = train_df[col].nunique()
            top_value = train_df[col].value_counts().index[0]
            top_count = train_df[col].value_counts().values[0]
            null_count = train_df[col].isnull().sum()
            
            print(f"\n   {col}:")
            print(f"      –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_values}")
            print(f"      –°–∞–º–æ–µ —á–∞—Å—Ç–æ–µ: {top_value} ({top_count/len(train_df)*100:.1f}%)")
            if null_count > 0:
                print(f"      {Fore.YELLOW}–ü—Ä–æ–ø—É—Å–∫–∏: {null_count} ({null_count/len(train_df)*100:.1f}%)")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print(f"{Fore.MAGENTA}{'='*80}")
    print(f"{Fore.MAGENTA}{' '*25}üí≥ LOAN APPROVAL PREDICTION")
    print(f"{Fore.MAGENTA}{' '*25}–®–∞–≥ 1: –ü–µ—Ä–≤–æ–µ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ")
    print(f"{Fore.MAGENTA}{'='*80}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞
    train_df, test_df = load_data()
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    analyze_structure(train_df, test_df)
    
    # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    analyze_target(train_df)
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyze_features(train_df)
    
    print(f"\n{Fore.MAGENTA}{'='*80}")
    print(f"{Fore.MAGENTA}{' '*30}‚úÖ –®–ê–ì 1 –ó–ê–í–ï–†–®–ï–ù!")
    print(f"{Fore.MAGENTA}{'='*80}")
    
    return train_df, test_df

if __name__ == "__main__":
    train_df, test_df = main()