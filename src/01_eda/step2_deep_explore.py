"""
===============================================================================
                    üí≥ LOAN APPROVAL PREDICTION
                    
                 –®–ê–ì 2: –î–ï–¢–ê–õ–¨–ù–û–ï –ò–ó–£–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í
                    "Deep Dive into Each Feature"
===============================================================================

–¶–ï–õ–¨ –≠–¢–û–ì–û –§–ê–ô–õ–ê:
-----------------
–î–µ—Ç–∞–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å –ö–ê–ñ–î–´–ô –ø—Ä–∏–∑–Ω–∞–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –í –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑ –º—ã —É–≤–∏–¥–µ–ª–∏
–æ–±—â—É—é –∫–∞—Ä—Ç–∏–Ω—É, —Ç–µ–ø–µ—Ä—å –∫–æ–ø–∞–µ–º –≥–ª—É–±–∂–µ - —Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –≤—ã–±—Ä–æ—Å—ã,
—Å–≤—è–∑—å —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.

–ß–¢–û –ú–´ –£–ó–ù–ê–ï–ú:
-------------
1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)
2. –í—ã–±—Ä–æ—Å—ã –∏ –∞–Ω–æ–º–∞–ª–∏–∏ (—Ç–µ —Å–∞–º—ã–µ 123 –≥–æ–¥–∞!)
3. –ö–∞–∫ –∫–∞–∂–¥—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –≤–ª–∏—è–µ—Ç –Ω–∞ –æ–¥–æ–±—Ä–µ–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞
4. –ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ

–í–ê–ñ–ù–û:
------
–°–æ–∑–¥–∞–¥–∏–º –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –ö–ê–ñ–î–û–ì–û –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º!
"""

# ==============================================================================
# –ò–ú–ü–û–†–¢–´
# ==============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from colorama import init, Fore, Style
import warnings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
warnings.filterwarnings('ignore')
init(autoreset=True)
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==============================================================================
# –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô
# ==============================================================================

# –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
ROOT_DIR = Path(__file__).parent.parent.parent
DATA_RAW = ROOT_DIR / 'data' / 'raw'
RESULTS = ROOT_DIR / 'results'

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç—Ç–æ–≥–æ —à–∞–≥–∞
STEP2_DIR = RESULTS / 'step2_deep_explore'
STEP2_DIR.mkdir(parents=True, exist_ok=True)

# ==============================================================================
# –§–£–ù–ö–¶–ò–Ø 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ==============================================================================

def load_data():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫—Ä–µ–¥–∏—Ç–∞—Ö.
    
    –ü–æ—á–µ–º—É –æ—Ç–¥–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è:
    - –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
    - –ï–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∑–¥–µ
    """
    print(f"\n{Fore.CYAN}üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ...")
    
    train_df = pd.read_csv(DATA_RAW / 'train.csv')
    test_df = pd.read_csv(DATA_RAW / 'test.csv')
    
    print(f"{Fore.GREEN}‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(train_df):,} train, {len(test_df):,} test")
    return train_df, test_df

# ==============================================================================
# –§–£–ù–ö–¶–ò–Ø 2: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
# ==============================================================================

def classify_features(train_df):
    """
    –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –≥—Ä—É–ø–ø—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
    
    –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ–º–æ–≥–∞–µ—Ç:
    - –ü–æ–Ω—è—Ç—å –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É
    - –ü—Ä–∏–º–µ–Ω—è—Ç—å —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - –°–æ–∑–¥–∞–≤–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ feature interactions
    """
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üìä –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í")
    print(f"{Fore.CYAN}{'='*80}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Å–º—ã—Å–ª—É
    personal_features = [
        'person_age',           # –í–æ–∑—Ä–∞—Å—Ç –∫–ª–∏–µ–Ω—Ç–∞
        'person_income',        # –ì–æ–¥–æ–≤–æ–π –¥–æ—Ö–æ–¥
        'person_home_ownership',# –¢–∏–ø –≤–ª–∞–¥–µ–Ω–∏—è –∂–∏–ª—å–µ–º
        'person_emp_length'     # –°—Ç–∞–∂ —Ä–∞–±–æ—Ç—ã
    ]
    
    loan_features = [
        'loan_intent',          # –¶–µ–ª—å –∫—Ä–µ–¥–∏—Ç–∞
        'loan_grade',           # –ì—Ä–µ–π–¥ –∫—Ä–µ–¥–∏—Ç–∞ (—Ä–∏—Å–∫)
        'loan_amnt',            # –°—É–º–º–∞ –∫—Ä–µ–¥–∏—Ç–∞
        'loan_int_rate',        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —Å—Ç–∞–≤–∫–∞
        'loan_percent_income'   # –ü–ª–∞—Ç–µ–∂/–¥–æ—Ö–æ–¥
    ]
    
    credit_history = [
        'cb_person_default_on_file',      # –ë—ã–ª –ª–∏ –¥–µ—Ñ–æ–ª—Ç
        'cb_person_cred_hist_length'       # –î–ª–∏–Ω–∞ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏
    ]
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
    numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_features = [f for f in numeric_features if f not in ['id', 'loan_status']]
    
    categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()
    
    print(f"\nüë§ –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(personal_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"üí∞ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫—Ä–µ–¥–∏—Ç–∞: {len(loan_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"üìä –ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è: {len(credit_history)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"\nüî¢ –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(numeric_features)}")
    print(f"üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(categorical_features)}")
    
    return {
        'personal': personal_features,
        'loan': loan_features,
        'credit': credit_history,
        'numeric': numeric_features,
        'categorical': categorical_features
    }

# ==============================================================================
# –§–£–ù–ö–¶–ò–Ø 3: –ê–ù–ê–õ–ò–ó –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# ==============================================================================

def analyze_numeric_features(train_df, numeric_features):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.
    
    –ß—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
    - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∏–ª–∏ —Å–∫–æ—à–µ–Ω–Ω–æ–µ?)
    - –í—ã–±—Ä–æ—Å—ã (IQR –º–µ—Ç–æ–¥)
    - –°–≤—è–∑—å —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    """
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üî¢ –ê–ù–ê–õ–ò–ó –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    print(f"{Fore.CYAN}{'='*80}")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_dir = STEP2_DIR / 'numeric_features'
    numeric_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
    numeric_stats = []
    
    for feature in numeric_features:
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {feature}...")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        feature_dir = numeric_dir / feature
        feature_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = train_df[feature].dropna()  # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        # ========== –°–ß–ò–¢–ê–ï–ú –°–¢–ê–¢–ò–°–¢–ò–ö–ò ==========
        stats = {
            'feature': feature,
            'count': len(data),
            'missing': train_df[feature].isnull().sum(),
            'missing_pct': train_df[feature].isnull().sum() / len(train_df) * 100,
            'mean': data.mean(),
            'median': data.median(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max(),
            'q25': data.quantile(0.25),
            'q75': data.quantile(0.75),
            'iqr': data.quantile(0.75) - data.quantile(0.25),
            'skewness': data.skew(),
            'kurtosis': data.kurtosis()
        }
        
        # ========== –ù–ê–•–û–î–ò–ú –í–´–ë–†–û–°–´ (IQR –º–µ—Ç–æ–¥) ==========
        Q1 = stats['q25']
        Q3 = stats['q75']
        IQR = stats['iqr']
        
        # –ì—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # –°—á–∏—Ç–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
        outliers = data[(data < lower_bound) | (data > upper_bound)]
        stats['outliers_count'] = len(outliers)
        stats['outliers_pct'] = len(outliers) / len(data) * 100
        
        # –û—Å–æ–±—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ —Å—Ç–∞–∂–∞ (—Ç–µ —Å–∞–º—ã–µ 123!)
        if feature in ['person_age', 'person_emp_length']:
            suspicious_123 = (train_df[feature] == 123).sum()
            if suspicious_123 > 0:
                stats['suspicious_123'] = suspicious_123
                print(f"   {Fore.YELLOW}‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {suspicious_123} –∑–Ω–∞—á–µ–Ω–∏–π = 123 (–≤–æ–∑–º–æ–∂–Ω–æ, –∫–æ–¥ –¥–ª—è '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')")
        
        numeric_stats.append(stats)
        
        # ========== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ==========
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å 4 –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å –ª–∏–Ω–∏—è–º–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ –º–µ–¥–∏–∞–Ω—ã
        axes[0, 0].hist(data, bins=30, edgecolor='black', alpha=0.7, color='steelblue')
        axes[0, 0].axvline(data.mean(), color='red', linestyle='--', 
                          linewidth=2, label=f'Mean: {data.mean():.1f}')
        axes[0, 0].axvline(data.median(), color='green', linestyle='--', 
                          linewidth=2, label=f'Median: {data.median():.1f}')
        axes[0, 0].set_title(f'{feature} - Distribution')
        axes[0, 0].set_xlabel(feature)
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: Boxplot –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤
        bp = axes[0, 1].boxplot(data, vert=True, patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
        bp['boxes'][0].set_alpha(0.7)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–æ—Å–∞—Ö
        axes[0, 1].set_title(f'{feature} - Boxplot\nOutliers: {stats["outliers_count"]} ({stats["outliers_pct"]:.1f}%)')
        axes[0, 1].set_ylabel(feature)
        axes[0, 1].grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏–∏ –¥–ª—è –∫–≤–∞—Ä—Ç–∏–ª–µ–π
        axes[0, 1].text(1.1, stats['min'], f'Min: {stats["min"]:.1f}', fontsize=9)
        axes[0, 1].text(1.1, stats['q25'], f'Q1: {stats["q25"]:.1f}', fontsize=9)
        axes[0, 1].text(1.1, stats['median'], f'Median: {stats["median"]:.1f}', fontsize=9)
        axes[0, 1].text(1.1, stats['q75'], f'Q3: {stats["q75"]:.1f}', fontsize=9)
        axes[0, 1].text(1.1, stats['max'], f'Max: {stats["max"]:.1f}', fontsize=9)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ loan_status
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –æ–¥–æ–±—Ä–µ–Ω–Ω—ã—Ö –∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã—Ö
        approved = train_df[train_df['loan_status'] == 1][feature].dropna()
        rejected = train_df[train_df['loan_status'] == 0][feature].dropna()
        
        axes[1, 0].hist([rejected, approved], bins=20, alpha=0.7, 
                       label=['Rejected', 'Approved'], color=['red', 'green'])
        axes[1, 0].set_title(f'{feature} by Loan Status')
        axes[1, 0].set_xlabel(feature)
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: Violin plot –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        df_plot = train_df[[feature, 'loan_status']].dropna()
        df_plot['Status'] = df_plot['loan_status'].map({0: 'Rejected', 1: 'Approved'})
        
        sns.violinplot(data=df_plot, x='Status', y=feature, ax=axes[1, 1])
        axes[1, 1].set_title(f'{feature} Distribution by Status')
        axes[1, 1].grid(True, alpha=0.3)
        
        # –û–±—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        fig.suptitle(f'–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {feature}', fontsize=14, y=1.02)
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.savefig(feature_dir / f'{feature}_analysis.png', dpi=100, bbox_inches='tight')
        plt.close()
        
        # ========== –°–û–•–†–ê–ù–Ø–ï–ú –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –§–ê–ô–õ ==========
        with open(feature_dir / f'{feature}_stats.txt', 'w') as f:
            f.write(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞: {feature}\n")
            f.write("="*50 + "\n\n")
            
            f.write("–û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:\n")
            f.write(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {stats['count']:,}\n")
            f.write(f"  –ü—Ä–æ–ø—É—Å–∫–∏: {stats['missing']:,} ({stats['missing_pct']:.1f}%)\n")
            f.write(f"  –°—Ä–µ–¥–Ω–µ–µ: {stats['mean']:.2f}\n")
            f.write(f"  –ú–µ–¥–∏–∞–Ω–∞: {stats['median']:.2f}\n")
            f.write(f"  Std: {stats['std']:.2f}\n")
            f.write(f"  Min: {stats['min']:.2f}\n")
            f.write(f"  Max: {stats['max']:.2f}\n")
            
            f.write("\n–ö–í–ê–†–¢–ò–õ–ò:\n")
            f.write(f"  Q1 (25%): {stats['q25']:.2f}\n")
            f.write(f"  Q2 (50%): {stats['median']:.2f}\n")
            f.write(f"  Q3 (75%): {stats['q75']:.2f}\n")
            f.write(f"  IQR: {stats['iqr']:.2f}\n")
            
            f.write("\n–§–û–†–ú–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø:\n")
            f.write(f"  Skewness: {stats['skewness']:.3f}\n")
            if abs(stats['skewness']) < 0.5:
                f.write("    ‚Üí –ë–ª–∏–∑–∫–æ –∫ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–º—É\n")
            elif stats['skewness'] > 0.5:
                f.write("    ‚Üí –°–∫–æ—à–µ–Ω–æ –≤–ø—Ä–∞–≤–æ (–¥–ª–∏–Ω–Ω—ã–π —Ö–≤–æ—Å—Ç —Å–ø—Ä–∞–≤–∞)\n")
            else:
                f.write("    ‚Üí –°–∫–æ—à–µ–Ω–æ –≤–ª–µ–≤–æ (–¥–ª–∏–Ω–Ω—ã–π —Ö–≤–æ—Å—Ç —Å–ª–µ–≤–∞)\n")
            
            f.write(f"  Kurtosis: {stats['kurtosis']:.3f}\n")
            if abs(stats['kurtosis']) < 1:
                f.write("    ‚Üí –ë–ª–∏–∑–∫–æ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É\n")
            elif stats['kurtosis'] > 1:
                f.write("    ‚Üí –û—Å—Ç—Ä—ã–π –ø–∏–∫ (leptokurtic)\n")
            else:
                f.write("    ‚Üí –ü–ª–æ—Å–∫–∏–π –ø–∏–∫ (platykurtic)\n")
            
            f.write("\n–í–´–ë–†–û–°–´:\n")
            f.write(f"  –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞: {lower_bound:.2f}\n")
            f.write(f"  –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞: {upper_bound:.2f}\n")
            f.write(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–æ—Å–æ–≤: {stats['outliers_count']:,}\n")
            f.write(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –≤—ã–±—Ä–æ—Å–æ–≤: {stats['outliers_pct']:.1f}%\n")
            
            if 'suspicious_123' in stats:
                f.write(f"\n‚ö†Ô∏è –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø:\n")
                f.write(f"  –ó–Ω–∞—á–µ–Ω–∏–π = 123: {stats['suspicious_123']}\n")
                f.write(f"  –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –∫–æ–¥ –¥–ª—è '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'\n")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    stats_df = pd.DataFrame(numeric_stats)
    stats_df.to_csv(STEP2_DIR / 'numeric_features_statistics.csv', index=False)
    
    print(f"\n{Fore.GREEN}‚úÖ –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {numeric_dir}")
    
    return stats_df

# ==============================================================================
# –§–£–ù–ö–¶–ò–Ø 4: –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# ==============================================================================

def analyze_categorical_features(train_df, categorical_features):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    –ß—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
    - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    - –†–µ–¥–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    - –°–≤—è–∑—å —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (approval rate –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º)
    """
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üìù –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    print(f"{Fore.CYAN}{'='*80}")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    categorical_dir = STEP2_DIR / 'categorical_features'
    categorical_dir.mkdir(parents=True, exist_ok=True)
    
    categorical_stats = []
    
    for feature in categorical_features:
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {feature}...")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞
        feature_dir = categorical_dir / feature
        feature_dir.mkdir(parents=True, exist_ok=True)
        
        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        value_counts = train_df[feature].value_counts()
        value_pcts = train_df[feature].value_counts(normalize=True) * 100
        
        stats = {
            'feature': feature,
            'unique_values': train_df[feature].nunique(),
            'missing': train_df[feature].isnull().sum(),
            'missing_pct': train_df[feature].isnull().sum() / len(train_df) * 100,
            'most_common': value_counts.index[0],
            'most_common_count': value_counts.values[0],
            'most_common_pct': value_pcts.values[0],
            'least_common': value_counts.index[-1],
            'least_common_count': value_counts.values[-1],
            'least_common_pct': value_pcts.values[-1]
        }
        
        categorical_stats.append(stats)
        
        # ========== –ê–ù–ê–õ–ò–ó –°–í–Ø–ó–ò –° –¶–ï–õ–ï–í–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô ==========
        # –°—á–∏—Ç–∞–µ–º approval rate –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        approval_by_category = train_df.groupby(feature)['loan_status'].agg(['mean', 'count'])
        approval_by_category.columns = ['approval_rate', 'count']
        approval_by_category['approval_rate'] *= 100  # –í –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        
        # ========== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ==========
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        value_counts.plot(kind='bar', ax=axes[0, 0], color='skyblue', edgecolor='black')
        axes[0, 0].set_title(f'{feature} - Distribution')
        axes[0, 0].set_xlabel(feature)
        axes[0, 0].set_ylabel('Count')
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # –ü–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –ø–æ–¥–ø–∏—Å–∏ –µ—Å–ª–∏ –∏—Ö –º–Ω–æ–≥–æ
        if len(value_counts) > 3:
            axes[0, 0].tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (pie chart)
        if len(value_counts) <= 10:  # Pie chart —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            axes[0, 1].pie(value_counts.values, labels=value_counts.index, 
                          autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title(f'{feature} - Percentage Distribution')
        else:
            # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –º–Ω–æ–≥–æ, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10
            top10 = value_counts.head(10)
            other = value_counts[10:].sum()
            if other > 0:
                plot_data = pd.concat([top10, pd.Series({'Other': other})])
            else:
                plot_data = top10
            axes[0, 1].pie(plot_data.values, labels=plot_data.index, 
                          autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title(f'{feature} - Top 10 Categories')
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Approval rate –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        approval_by_category['approval_rate'].plot(kind='bar', ax=axes[1, 0], 
                                                   color='green', edgecolor='black', alpha=0.7)
        axes[1, 0].axhline(y=train_df['loan_status'].mean() * 100, 
                          color='red', linestyle='--', 
                          label=f'Overall: {train_df["loan_status"].mean()*100:.1f}%')
        axes[1, 0].set_title(f'{feature} - Approval Rate by Category')
        axes[1, 0].set_xlabel(feature)
        axes[1, 0].set_ylabel('Approval Rate (%)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        if len(approval_by_category) > 3:
            axes[1, 0].tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: Stacked bar chart (approved vs rejected)
        crosstab = pd.crosstab(train_df[feature], train_df['loan_status'], normalize='index') * 100
        crosstab.plot(kind='bar', stacked=True, ax=axes[1, 1], 
                     color=['red', 'green'], alpha=0.7, edgecolor='black')
        axes[1, 1].set_title(f'{feature} - Approved vs Rejected')
        axes[1, 1].set_xlabel(feature)
        axes[1, 1].set_ylabel('Percentage')
        axes[1, 1].legend(['Rejected', 'Approved'])
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        if len(crosstab) > 3:
            axes[1, 1].tick_params(axis='x', rotation=45)
        
        fig.suptitle(f'–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {feature}', fontsize=14, y=1.02)
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        plt.savefig(feature_dir / f'{feature}_analysis.png', dpi=100, bbox_inches='tight')
        plt.close()
        
        # ========== –°–û–•–†–ê–ù–Ø–ï–ú –î–ï–¢–ê–õ–¨–ù–£–Æ –°–¢–ê–¢–ò–°–¢–ò–ö–£ ==========
        with open(feature_dir / f'{feature}_stats.txt', 'w') as f:
            f.write(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞: {feature}\n")
            f.write("="*50 + "\n\n")
            
            f.write("–û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:\n")
            f.write(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {stats['unique_values']}\n")
            f.write(f"  –ü—Ä–æ–ø—É—Å–∫–∏: {stats['missing']} ({stats['missing_pct']:.1f}%)\n")
            f.write(f"\n–°–ê–ú–û–ï –ß–ê–°–¢–û–ï –ó–ù–ê–ß–ï–ù–ò–ï:\n")
            f.write(f"  {stats['most_common']}: {stats['most_common_count']:,} ({stats['most_common_pct']:.1f}%)\n")
            f.write(f"\n–°–ê–ú–û–ï –†–ï–î–ö–û–ï –ó–ù–ê–ß–ï–ù–ò–ï:\n")
            f.write(f"  {stats['least_common']}: {stats['least_common_count']:,} ({stats['least_common_pct']:.1f}%)\n")
            
            f.write("\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ù–ê–ß–ï–ù–ò–ô:\n")
            for val, count in value_counts.items():
                pct = count / len(train_df) * 100
                f.write(f"  {val:20}: {count:6,} ({pct:5.1f}%)\n")
            
            f.write("\nAPPROVAL RATE –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:\n")
            for idx, row in approval_by_category.iterrows():
                f.write(f"  {idx:20}: {row['approval_rate']:5.1f}% (n={row['count']:,})\n")
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —Å–∞–º—ã–º –≤—ã—Å–æ–∫–∏–º –∏ –Ω–∏–∑–∫–∏–º approval rate
            best_category = approval_by_category['approval_rate'].idxmax()
            worst_category = approval_by_category['approval_rate'].idxmin()
            
            f.write(f"\nüìà –õ—É—á—à–∏–π approval rate: {best_category} ({approval_by_category.loc[best_category, 'approval_rate']:.1f}%)\n")
            f.write(f"üìâ –•—É–¥—à–∏–π approval rate: {worst_category} ({approval_by_category.loc[worst_category, 'approval_rate']:.1f}%)\n")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    cat_stats_df = pd.DataFrame(categorical_stats)
    cat_stats_df.to_csv(STEP2_DIR / 'categorical_features_statistics.csv', index=False)
    
    print(f"\n{Fore.GREEN}‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {categorical_dir}")
    
    return cat_stats_df

# ==============================================================================
# –§–£–ù–ö–¶–ò–Ø 5: –ê–ù–ê–õ–ò–ó –°–í–Ø–ó–ï–ô –ú–ï–ñ–î–£ –ü–†–ò–ó–ù–ê–ö–ê–ú–ò
# ==============================================================================

def analyze_feature_relationships(train_df, features_dict):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.
    
    –ß—Ç–æ –∏—â–µ–º:
    - –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    - –°–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –¥–ª—è loan_status
    - –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å
    """
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üîó –ê–ù–ê–õ–ò–ó –°–í–Ø–ó–ï–ô –ú–ï–ñ–î–£ –ü–†–ò–ó–ù–ê–ö–ê–ú–ò")
    print(f"{Fore.CYAN}{'='*80}")
    
    numeric_features = features_dict['numeric']
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
    corr_matrix = train_df[numeric_features + ['loan_status']].corr()
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    target_corr = corr_matrix['loan_status'].drop('loan_status').sort_values(ascending=False)
    
    print(f"\nüéØ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å loan_status:")
    print("-"*50)
    for feature, corr in target_corr.items():
        if abs(corr) > 0.1:
            strength = "üî• –°–ò–õ–¨–ù–ê–Ø" if abs(corr) > 0.3 else "‚ö° –°–†–ï–î–ù–Ø–Ø" if abs(corr) > 0.2 else "üí® –°–õ–ê–ë–ê–Ø"
            print(f"  {feature:30}: {corr:+.4f} {strength}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
    plt.figure(figsize=(12, 10))
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    
    # Heatmap
    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', 
                cmap='coolwarm', center=0, square=True,
                linewidths=0.5, cbar_kws={"shrink": 0.8})
    
    plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontsize=14, pad=20)
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    plt.savefig(STEP2_DIR / 'correlation_matrix.png', dpi=100, bbox_inches='tight')
    plt.close()
    
    # –ò—â–µ–º –ø–∞—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (–º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å)
    high_corr_pairs = []
    for i in range(len(numeric_features)):
        for j in range(i+1, len(numeric_features)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:
                high_corr_pairs.append({
                    'feature1': numeric_features[i],
                    'feature2': numeric_features[j],
                    'correlation': corr_value
                })
    
    if high_corr_pairs:
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –ø–∞—Ä—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (>0.7):")
        for pair in high_corr_pairs:
            print(f"  {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}")
        print(f"  ‚Üí –í–æ–∑–º–æ–∂–Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    else:
        print(f"\n{Fore.GREEN}‚úÖ –í—ã—Å–æ–∫–∏—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    target_corr.to_csv(STEP2_DIR / 'correlations_with_target.csv')
    
    return corr_matrix, target_corr

# ==============================================================================
# –§–£–ù–ö–¶–ò–Ø 6: –°–û–ó–î–ê–ù–ò–ï –ò–¢–û–ì–û–í–û–ì–û –û–¢–ß–ï–¢–ê
# ==============================================================================

def create_summary_report(numeric_stats_df, categorical_stats_df, target_corr):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª.
    """
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}üìã –°–û–ó–î–ê–ù–ò–ï –ò–¢–û–ì–û–í–û–ì–û –û–¢–ß–ï–¢–ê")
    print(f"{Fore.CYAN}{'='*80}")
    
    with open(STEP2_DIR / 'SUMMARY_REPORT.txt', 'w') as f:
        f.write("="*70 + "\n")
        f.write(" "*20 + "LOAN APPROVAL PREDICTION\n")
        f.write(" "*15 + "–®–∞–≥ 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n")
        f.write("="*70 + "\n\n")
        
        f.write("–ö–õ–Æ–ß–ï–í–´–ï –ù–ê–•–û–î–ö–ò:\n")
        f.write("-"*50 + "\n\n")
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        f.write("1. –ü–†–û–ë–õ–ï–ú–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:\n")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è 123
        age_123 = numeric_stats_df[numeric_stats_df['feature'] == 'person_age']['suspicious_123'].values
        emp_123 = numeric_stats_df[numeric_stats_df['feature'] == 'person_emp_length']['suspicious_123'].values
        
        if len(age_123) > 0 and age_123[0] > 0:
            f.write(f"   ‚Ä¢ person_age: {int(age_123[0])} –∑–Ω–∞—á–µ–Ω–∏–π = 123 (–∞–Ω–æ–º–∞–ª–∏—è)\n")
        if len(emp_123) > 0 and emp_123[0] > 0:
            f.write(f"   ‚Ä¢ person_emp_length: {int(emp_123[0])} –∑–Ω–∞—á–µ–Ω–∏–π = 123 (–∞–Ω–æ–º–∞–ª–∏—è)\n")
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º % –≤—ã–±—Ä–æ—Å–æ–≤
        high_outliers = numeric_stats_df[numeric_stats_df['outliers_pct'] > 5]
        if not high_outliers.empty:
            f.write(f"\n   –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã–±—Ä–æ—Å–æ–≤ (>5%):\n")
            for _, row in high_outliers.iterrows():
                f.write(f"   ‚Ä¢ {row['feature']}: {row['outliers_pct']:.1f}% –≤—ã–±—Ä–æ—Å–æ–≤\n")
        
        # –¢–æ–ø –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã
        f.write("\n2. –°–ê–ú–´–ï –í–ê–ñ–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (–ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å loan_status):\n")
        for i, (feature, corr) in enumerate(target_corr.head(5).items(), 1):
            f.write(f"   {i}. {feature}: {corr:+.4f}\n")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        f.write("\n3. –û–°–û–ë–ï–ù–ù–û–°–¢–ò –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ô:\n")
        
        # –°–∫–æ—à–µ–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        highly_skewed = numeric_stats_df[abs(numeric_stats_df['skewness']) > 2]
        if not highly_skewed.empty:
            f.write("   –°–∏–ª—å–Ω–æ —Å–∫–æ—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (|skew| > 2):\n")
            for _, row in highly_skewed.iterrows():
                f.write(f"   ‚Ä¢ {row['feature']}: skewness = {row['skewness']:.2f}\n")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        f.write("\n4. –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:\n")
        for _, row in categorical_stats_df.iterrows():
            f.write(f"   ‚Ä¢ {row['feature']}: {row['unique_values']} –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        f.write("\n5. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø PREPROCESSING:\n")
        f.write("   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è 123 –≤ person_age –∏ person_emp_length\n")
        f.write("   ‚Ä¢ –ü—Ä–∏–º–µ–Ω–∏—Ç—å log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ person_income (—Å–∏–ª—å–Ω–æ —Å–∫–æ—à–µ–Ω)\n")
        f.write("   ‚Ä¢ One-hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n")
        f.write("   ‚Ä¢ Ordinal encoding –¥–ª—è loan_grade (A‚ÜíG –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω)\n")
        f.write("   ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (ratios, interactions)\n")
        f.write("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å class_weight='balanced' –∏–∑-–∑–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ (14% approval)\n")
    
    print(f"{Fore.GREEN}‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {STEP2_DIR / 'SUMMARY_REPORT.txt'}")

# ==============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ==============================================================================

def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–ø—É—Å–∫–∞–µ—Ç –≤–µ—Å—å –∞–Ω–∞–ª–∏–∑.
    """
    print(f"{Fore.MAGENTA}{'='*80}")
    print(f"{Fore.MAGENTA}{' '*25}üí≥ LOAN APPROVAL PREDICTION")
    print(f"{Fore.MAGENTA}{' '*22}–®–∞–≥ 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    print(f"{Fore.MAGENTA}{'='*80}")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    train_df, test_df = load_data()
    
    # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features_dict = classify_features(train_df)
    
    # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numeric_stats_df = analyze_numeric_features(train_df, features_dict['numeric'])
    
    # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    categorical_stats_df = analyze_categorical_features(train_df, features_dict['categorical'])
    
    # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    corr_matrix, target_corr = analyze_feature_relationships(train_df, features_dict)
    
    # 6. –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    create_summary_report(numeric_stats_df, categorical_stats_df, target_corr)
    
    print(f"\n{Fore.MAGENTA}{'='*80}")
    print(f"{Fore.MAGENTA}{' '*30}‚úÖ –®–ê–ì 2 –ó–ê–í–ï–†–®–ï–ù!")
    print(f"{Fore.MAGENTA}{'='*80}")
    
    print(f"\n{Fore.YELLOW}üìå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    print(f"   {STEP2_DIR}")
    
    print(f"\n{Fore.YELLOW}üìå –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
    print(f"   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: {Fore.CYAN}python src/01_eda/step3_check_quality.py")
    print(f"   –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–±–ª–µ–º")
    
    return train_df, test_df, features_dict

# ==============================================================================
# –¢–û–ß–ö–ê –í–•–û–î–ê
# ==============================================================================

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    train_df, test_df, features_dict = main()